<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tutorial Step 3: Training &#8212; bfbrain 1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=039e1c02" />
    <script src="_static/documentation_options.js?v=f2a433a1"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tutorial Step 4: Analysis" href="analysis.html" />
    <link rel="prev" title="Tutorial Step 2: Initializing the Classifier" href="classifier.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="tutorial-step-3-training">
<span id="training"></span><h1>Tutorial Step 3: Training<a class="headerlink" href="#tutorial-step-3-training" title="Link to this heading">¶</a></h1>
<p>With the <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a> object and all performance metrics initialized, a single line of code can now be used to perform the active learning loop for a
specified number of iterations via the <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a> method.
With the example <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a>
that we implemented in <a class="reference internal" href="classifier.html#classifier"><span class="std std-ref">the last tutorial section</span></a>, which we named AL, a typical call to <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a> might be</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AL</span><span class="o">.</span><span class="n">AL_loop</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">&#39;saved_AL&#39;</span><span class="p">,</span> <span class="n">nstop</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
<p>All inputs to <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span> <span class="pre">method</span></code></a> possess default values and can therefore be called for a wide variety of scalar potentials without <em>any</em> user input, however for clarity
we have specified the two most important arguments above. Given the sheer number of options that are exposed to the user in <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span> <span class="pre">method</span></code></a>, it is recommended
that a user always specify keyword, rather than positional, arguments when invoking this method.</p>
<p>The first argument we have supplied, filepath, gives the name under which the <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a> object will be saved after
the active learning loop completes. After active learning, a directory under this name will appear in the Python script’s directory which is of the appropriate form to load a <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a>
object in a later session. The second argument we have specified, nstop, will specify the number of active learning iterations that the model should perform– in this case, 20– before terminating.</p>
<p>If a reader has been following along, running this code can take as long as two hours to complete on a personal laptop with a single GPU. For demonstrative purposes, we may recommend running the following code,
which will perform the same active learning training loop but terminates after just two rounds of active learning, allowing the run to complete in a matter of minutes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AL</span><span class="o">.</span><span class="n">AL_loop</span><span class="p">(</span><span class="n">filepath</span> <span class="o">=</span> <span class="s1">&#39;saved_AL&#39;</span><span class="p">,</span> <span class="n">nstop</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>At the end of active learning, our ‘saved_AL’ directory will have the structure</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>saved_AL
├── data_train.npz
├── data_val.npz
├── dm.pickle
├── history.pickle
├── history_accuracy.png
├── history_loss.png
├── metrics.pickle
├── model
│   ├── assets
│   ├── fingerprint.pb
│   ├── keras_metadata.pb
│   ├── saved_model.pb
│   └── variables
│       ├── variables.data-00000-of-00001
│       └── variables.index
├── model_delta_F.png
├── output.txt
├── val_BALD_fscore.png
└── variables.pickle
</pre></div>
</div>
<p>Most of the information in the structure above (in particular model as well as all .npz and .pickle files) are used by BFBrain to load the <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a> object from
a directory. However, some of the outputs contain summary information about the outcome of training.
First, ‘output.txt’ contains entries which tracks the results of all performance metrics in all rounds. As an example, the execution of our example program has yielded the entry</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Metrics</span> <span class="k">for</span> <span class="nb">round</span> <span class="mi">20</span><span class="p">:</span>
<span class="n">val_BALD_fscore</span> <span class="p">(</span><span class="n">validation</span> <span class="n">precision</span><span class="p">)</span> <span class="p">[</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]:</span>
<span class="p">[</span><span class="mf">0.9975669099756691</span><span class="p">,</span> <span class="mf">0.9953810623556582</span><span class="p">]</span>
<span class="n">val_BALD_fscore</span> <span class="p">(</span><span class="n">validation</span> <span class="n">recall</span><span class="p">)</span> <span class="p">[</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]:</span>
<span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.9795454545454545</span><span class="p">]</span>
<span class="n">val_BALD_fscore</span> <span class="p">(</span><span class="n">validation</span> <span class="n">F</span> <span class="n">score</span><span class="p">)</span> <span class="p">[</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]:</span>
<span class="p">[</span><span class="mf">0.9987819732034106</span><span class="p">,</span> <span class="mf">0.9873997709049255</span><span class="p">]</span>
<span class="n">model_delta_F</span><span class="p">:</span>
<span class="mf">0.02820636451301828</span>
</pre></div>
</div>
<p>as the results for the final round’s metric evaluations. Meanwhile, model_delta_F.png and val_BALD_fscore.png give simple plots of the results of the performance metrics:</p>
<a class="reference internal image-reference" href="_images/model_delta_F.png"><img alt="The results of plotting the :class:`UnlabelledDeltaF &lt;bfbrain.AL_Metrics.UnlabelledDeltaF&gt;` metric using the :meth:`plot_metric &lt;bfbrain.AL_Metrics.ALMetric.plot_metric&gt;` method, automatically generated by :meth:`AL_loop &lt;bfbrain.BFB_Learner.BFBLearner.AL_loop&gt;`." src="_images/model_delta_F.png" style="width: 400px;" /></a>
<a class="reference internal image-reference" href="_images/val_BALD_fscore.png"><img alt="The results of plotting the :class:`ValidationFScore &lt;bfbrain.AL_Metrics.ValidationFScore&gt;` metric using the :meth:`plot_metric &lt;bfbrain.AL_Metrics.ALMetric.plot_metric&gt;` method, automatically generated by :meth:`AL_loop &lt;bfbrain.BFB_Learner.BFBLearner.AL_loop&gt;`." src="_images/val_BALD_fscore.png" style="width: 400px;" /></a>
<p>Finally, principally for debugging purposes, BFBrain saves and plots the training loss and binary accuracy (extracted from the history object returned by Tensorflow’s model.fit() method) across all training epochs–
users knowledgeable in machine learning may wish to inspect these results (in history_loss.png and history_accuracy.png, respectively) to ensure that their neural network is achieving a high degree of accuracy on
the training data <a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>*<span class="fn-bracket">]</span></a></p>
<p>After an active learning loop has been completed, anyone with access to the filepath directory (or a copy thereof) can load the <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a> object
in order to continue training or use the model in analysis. Furthermore, training can be continued arbitrarily by repeated calls of <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span> <span class="pre">method</span></code></a>: A repetition
of the code block above will train the existing <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a> object for <em>another</em> 20 rounds, and overwrite the saved information in the directory ‘saved_AL’ with the new
<a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a> object which has now been trained for 40 rounds of active learning.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">*</a><span class="fn-bracket">]</span></span>
<p>A user who inspects these may be concerned that the loss and binary accuracy at the epoch level look highly oscillatory in later active learning rounds– this is a consequence of the large number of highly uncertain inputs in the training set, and we haven’t found it to correspond to a degradation of trained classifier performance, since the optimizer will still arrive at weights close to those which minimize the loss.</p>
</aside>
</aside>
<section id="options-in-the-active-learning-loop">
<span id="customal"></span><h2>Options in the Active Learning Loop<a class="headerlink" href="#options-in-the-active-learning-loop" title="Link to this heading">¶</a></h2>
<p><a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a> exposes a number of options to the user in order to customize the active learning loop– these range from altering the method by which
the program evaluates new points to be added to the training set, changing how many new training points are generated at each active learning iteration, and altering the size of data batches that are passed
simultaneously to the GPU during training and validation. A comprehensive review of all possible options for the method is included in <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">its</span> <span class="pre">documentation</span></code></a>, and
readers interested in experimenting with all of BFBrain’s capabilities are encouraged to read it thoroughly. For convenience, however, here we shall also list the options that a user is most likely to find useful,
or may need to change from their default values in order to meet the requirements of their analysis or machine:</p>
<ul class="simple">
<li><p><strong>K_batch_size, K_batch_num</strong>: These two integer parameters specify the number of new data points that should be added to the training set during each active learning iteration. A total of K_batch_num batches of K_batch_size points are generated and combined to form each additional set of training points, so at each active learning iteration, a total of K_batch_num*K_batch_size points are appended to the training set. By default, K_batch_size=500 and K_batch_num=10, so a total of 5000 points are added to the training set during each round of active learning.</p></li>
<li><p><strong>K_factor</strong>: Each batch of K_batch_size training points is found by drawing the points which the model is most uncertain about from a randomly generated pool of K_factor*K_batch_size points. If the user wants to sample a larger pool of candidate points (and therefore expect to get more uncertain training inputs), this is achieved by using a larger value of K_factor. Thought of another way, AL_loop will always select the top 1/K_factor quantile of points in its pool of candidate points to add to the training data set. It is important to note that as part of active learning loop, batches of K_batch_size*K_factor points will be simultaneously passed to the GPU– if the user is encountering OOM errors during the execution of the training loop with a particular value of K_factor, it is recommended to reduce K_batch_size and increase K_batch_num. By default, K_factor=100, so the active learning loop will select 5000 new training points at each active learning iteration out of a pool of 500000 candidates.</p></li>
<li><p><strong>score_fn</strong>: This specifies the method by which the active learning function estimates uncertainty in order to select points to add to the training set– we shall discuss this option in detail in <a class="reference internal" href="#score-fns"><span class="std std-ref">the next section</span></a>.</p></li>
<li><p><strong>batch_size, val_batch_size</strong>: These integers specify the number of training (batch_size) and validation (val_batch_size) examples that should be passed to the GPU at one time during training and validation. In order to minimize noise in training, it is recommended if possible to specify batch_size greater than the maximum total number of points in the training data that the algorithm will encounter (so, for 20 rounds of active learning, each producing 5000 points, it is recommended to have batch_size &gt; <span class="math notranslate nohighlight">\(10^5\)</span>, plus the size of the initial training data). If this is not possible due to memory constraints, it is recommended to instead use batches that are approximately equal to the number of new training points added during each round, so that about 1 new batch is added during each training set– otherwise sudden large changes in the number of batches late in training can result in sudden and significant degradation of performance. By default batch_size=:math:<cite>2 times 10^5</cite> and val_batch_size will always be equal to batch_size unless specified otherwise.</p></li>
<li><p><strong>verbose</strong>: If a user wishes the information in output.txt to also be printed to the console, they can specify verbose=True</p></li>
<li><p><strong>plot_metrics</strong>: If a user wishes the active learning loop to produce the plots generated by <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.ALMetric.plot_metric" title="bfbrain.AL_Metrics.ALMetric.plot_metric"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot_metric</span></code></a> to be printed to the console in addition to being saved after training, they can specify plot_metrics=True.</p></li>
</ul>
</section>
<section id="score-fn-uncertainty-quantification-options">
<span id="score-fns"></span><h2>score_fn: Uncertainty Quantification Options<a class="headerlink" href="#score-fn-uncertainty-quantification-options" title="Link to this heading">¶</a></h2>
<p>The parameter score_fn in <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a> allows a user to specify different methods for the active learning loop to estimate its uncertainty regarding potential training
points. Essentially, a function specified by the score_fn argument will assign a single “uncertainty score” to each input in a pool of candidate points, and then <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a>
will add points with very large uncertainty scores relative to the pool as a whole to the training set for the next round of active learning. In <a class="footnote-reference brackets" href="#id14" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, we found no significant variation in the results
from implementing different uncertainty-based strategies under normal use conditions, but a user may wish to experiment with different strategies for their particular use case.</p>
<p>BFBrain contains a number of built-in options for uncertainty quantification in <a class="reference internal" href="bfbrain.html#module-bfbrain.Score_Functions" title="bfbrain.Score_Functions"><code class="xref py py-mod docutils literal notranslate"><span class="pre">the</span> <span class="pre">Score_Functions</span> <span class="pre">module</span></code></a>, and they may be freely specified by string arguments for score_fn.
Because the BFBLearner’s model is a binary classification neural network with Monte Carlo dropout <a class="footnote-reference brackets" href="#id15" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, its predictions are computed by passing the same inputs through the neural network a number of times
and considering the ensemble of outputs– since different neurons are dropped out with each iteration, the model predicts different outputs for each forward pass of the same inputs.
Readers interested in the theory of various uncertainty estimates are referred to <a class="footnote-reference brackets" href="#id14" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, which discusses many of these in detail, as well as the references cited in each entry– we simply list the methods here, along
with linking to their implementations in <a class="reference internal" href="bfbrain.html#module-bfbrain.Score_Functions" title="bfbrain.Score_Functions"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Score_Functions</span></code></a>.</p>
<ul class="simple">
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.Score_Functions.BALD" title="bfbrain.Score_Functions.BALD"><code class="xref py py-mod docutils literal notranslate"><span class="pre">'BALD'</span></code></a>: Bayesian Active Learning by Disagreement (BALD) <a class="footnote-reference brackets" href="#id16" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, in which points are scored by mutual information <a class="footnote-reference brackets" href="#id17" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>– which is proposed in <a class="footnote-reference brackets" href="#id18" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> as an information-theoretic estimate of <em>epistemic uncertainty</em> , that is, uncertainty purely related to the model’s lack of training data and not the inherent ambiguity of a given training example. This is the default value for score_fn.</p></li>
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.Score_Functions.Max_Entropy" title="bfbrain.Score_Functions.Max_Entropy"><code class="xref py py-mod docutils literal notranslate"><span class="pre">'MaxEntropy'</span></code></a>: Shannon Entropy <a class="footnote-reference brackets" href="#id17" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, which is proposed in <a class="footnote-reference brackets" href="#id18" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> as an information-theoretic estimate of total predictive uncertainty.</p></li>
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.Score_Functions.Variation_Ratios" title="bfbrain.Score_Functions.Variation_Ratios"><code class="xref py py-mod docutils literal notranslate"><span class="pre">'variation_ratios'</span></code></a> : Variation ratios, which is defined as the fraction of forward passes through the neural network for which the model gives the <em>opposite</em> of the mode classification for a given input. By definition, this must be between 0. and 0.5.</p></li>
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.Score_Functions.Predictive_Variance" title="bfbrain.Score_Functions.Predictive_Variance"><code class="xref py py-mod docutils literal notranslate"><span class="pre">'predictive_variance'</span></code></a>: The standard deviation of the neural network’s numerical output. Suggested to correspond to epistemic uncertainty in <a class="footnote-reference brackets" href="#id19" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>.</p></li>
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.Score_Functions.QBDC" title="bfbrain.Score_Functions.QBDC"><code class="xref py py-mod docutils literal notranslate"><span class="pre">'QBDC'</span></code></a>: Query by Dropout Committee <a class="footnote-reference brackets" href="#id20" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>, in which points where the average output of all forward passes for a given input are closest to the classification threshold are scored the highest.</p></li>
</ul>
<p>Because each of the above scores are evaluated by making a large number of forward passes of given inputs through the metric, a user can also control how many forward passes are made with each method by specifying
the argument score_ntrials in <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a>. If this argument is not specified, default values listed in each method’s documentation are used.</p>
<p>Additionally, a score_fn input of ‘random’ will have <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a> assign random scores to each point in a pool of candidates– this can be used to demonstrate that
various active learning strategies are significantly outperforming random sampling of the parameter space for training.</p>
<p>Finally, a user may also implement custom score functions in the event that they would like to try active learning strategies not discussed here. In practice, this can be done by specifying any function with the signature
(tf.keras.Model, tf.Tensor(tf.float32, tf.float32)) -&gt; tf.Tensor(tf.float32) for score_fn, that is, any function which takes a Tensorflow model (in practice a <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner.model" title="bfbrain.BFB_Learner.BFBLearner.model"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BFBLearner.model</span></code></a>)
and a 2-dimensional Tensorflow tensor representing a batch of input points for the the model, and returns a 1-dimensional Tensorflow tensor of scores, one for each input. The function may also take a keyword integer argument, n_trials,
which in BFBrain’s implemented functions specifies the number of forward passes each input should make through the metric to extract the score, however if the score_ntrials argument of <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a>
is not specified, it is not necessary that a custom score_fn process this argument. The source code within <a class="reference internal" href="bfbrain.html#module-bfbrain.Score_Functions" title="bfbrain.Score_Functions"><code class="xref py py-mod docutils literal notranslate"><span class="pre">the</span> <span class="pre">Score_Functions</span> <span class="pre">module</span></code></a>, in which all of BFBrain’s existing
score functions are stored, provides a useful template for a user who may wish to implement any custom inputs for score_fn. The sole additional precaution that a user must take with any custom score_fn is that it must be
jit-compilable with tf.function– Tensorflow’s method for converting Python code into compilable scripts. Please see Tensorflow’s documentation for restrictions regarding the use of tf.function– in practice
most code that involves purely Tensorflow operations and basic loops and conditionals will usually fit the requirements.</p>
</section>
<section id="advanced-usage-stopping-criteria">
<h2>Advanced Usage: Stopping Criteria<a class="headerlink" href="#advanced-usage-stopping-criteria" title="Link to this heading">¶</a></h2>
<p>Before moving on to the use of a BFBrain-generated model in analysis, we briefly touch on one type of advanced usage in <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a>: Customized stopping criteria for active
learning. In <a class="footnote-reference brackets" href="#id14" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, we found no significant detriment (in terms of performance of the classifiers on validation sets) associated with continuing active learning for an arbitrary number of iterations, and since training time
should only scale linearly with increasing active learning iterations, we advise most users to simply continue active learning for as many rounds at one time as feasible, and if performance on a validation set
or on another metric, such as estimated <span class="math notranslate nohighlight">\(\Delta F_1\)</span> is unsatisfactory, to simply load the trained classifier and train the network for that many rounds again. Some users may, however, want to stop active learning only
once a predefined criterion based on performance metrics is reached. To that end, we implement a specialized class <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition" title="bfbrain.AL_Metrics.StoppingCondition"><code class="xref py py-class docutils literal notranslate"><span class="pre">StoppingCondition</span></code></a>, which can be passed to
<a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a> via the keyword argument stopping_cond.</p>
<p>A <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition" title="bfbrain.AL_Metrics.StoppingCondition"><code class="xref py py-class docutils literal notranslate"><span class="pre">StoppingCondition</span></code></a> instance will monitor a <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.ALMetric" title="bfbrain.AL_Metrics.ALMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">performance</span> <span class="pre">metric</span></code></a> that belongs to the <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-meth docutils literal notranslate"><span class="pre">BFBLearner</span></code></a>
object that is being trained in <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a>, specified in the <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition" title="bfbrain.AL_Metrics.StoppingCondition"><code class="xref py py-class docutils literal notranslate"><span class="pre">StoppingCondition</span></code></a> with the performance metric’S
<a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.ALMetric.name" title="bfbrain.AL_Metrics.ALMetric.name"><code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span> <span class="pre">string</span></code></a>. Whenever that performance metric is updated, the <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition" title="bfbrain.AL_Metrics.StoppingCondition"><code class="xref py py-class docutils literal notranslate"><span class="pre">StoppingCondition</span></code></a> object will call a method,
<a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition.metric_func" title="bfbrain.AL_Metrics.StoppingCondition.metric_func"><code class="xref py py-attr docutils literal notranslate"><span class="pre">metric_func</span></code></a> on the metric– if that function returns True, active learning will stop, while if that function returns False, active learning continues.</p>
<p>For user convenience, several stopping criteria are implemented as child classes to <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition" title="bfbrain.AL_Metrics.StoppingCondition"><code class="xref py py-class docutils literal notranslate"><span class="pre">StoppingCondition</span></code></a>, which monitor different types of metrics. These are</p>
<ul class="simple">
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.ScoreNotDecreasing" title="bfbrain.AL_Metrics.ScoreNotDecreasing"><code class="xref py py-class docutils literal notranslate"><span class="pre">ScoreNotDecreasing</span></code></a> monitors <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.PoolScore" title="bfbrain.AL_Metrics.PoolScore"><code class="xref py py-class docutils literal notranslate"><span class="pre">PoolScore</span></code></a> or <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.NewDataScore" title="bfbrain.AL_Metrics.NewDataScore"><code class="xref py py-class docutils literal notranslate"><span class="pre">NewDataScore</span></code></a> metrics and stops active learning if the uncertainty scores recorded by these metrics have not achieved a new minimum for some user-specified number of rounds, given by the attribute <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.ScoreNotDecreasing.patience" title="bfbrain.AL_Metrics.ScoreNotDecreasing.patience"><code class="xref py py-attr docutils literal notranslate"><span class="pre">patience</span></code></a>. This stopping condition should only be used for uncertainty metrics for which the uncertainty value should decrease with more training data– in BFBrain these are <a class="reference internal" href="bfbrain.html#bfbrain.Score_Functions.BALD" title="bfbrain.Score_Functions.BALD"><code class="xref py py-meth docutils literal notranslate"><span class="pre">BALD</span></code></a>, <a class="reference internal" href="bfbrain.html#bfbrain.Score_Functions.Variation_Ratios" title="bfbrain.Score_Functions.Variation_Ratios"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Variation_Ratios</span></code></a>, and <a class="reference internal" href="bfbrain.html#bfbrain.Score_Functions.Predictive_Variance" title="bfbrain.Score_Functions.Predictive_Variance"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Predictive_Variance</span></code></a>.</p></li>
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.AccuracyNotImproving" title="bfbrain.AL_Metrics.AccuracyNotImproving"><code class="xref py py-class docutils literal notranslate"><span class="pre">AccuracyNotImproving</span></code></a> monitors <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.ModelEvaluation" title="bfbrain.AL_Metrics.ModelEvaluation"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelEvaluation</span></code></a> or <code class="xref py py-class docutils literal notranslate"><span class="pre">MCModelEvaluation</span></code>.</p></li>
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.FScoreNotImproving" title="bfbrain.AL_Metrics.FScoreNotImproving"><code class="xref py py-class docutils literal notranslate"><span class="pre">FScoreNotImproving</span></code></a> works in the same way as <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.AccuracyNotImproving" title="bfbrain.AL_Metrics.AccuracyNotImproving"><code class="xref py py-class docutils literal notranslate"><span class="pre">AccuracyNotImproving</span></code></a>, but monitors the <span class="math notranslate nohighlight">\(F_1\)</span> score on a validation set extracted from a <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.ValidationFScore" title="bfbrain.AL_Metrics.ValidationFScore"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValidationFScore</span></code></a> or <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.ValidationConfusionMatrix" title="bfbrain.AL_Metrics.ValidationConfusionMatrix"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValidationConfusionMatrix</span></code></a> metric.</p></li>
<li><p><a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.DeltaFNotDecreasing" title="bfbrain.AL_Metrics.DeltaFNotDecreasing"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeltaFNotDecreasing</span></code></a> monitors a <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.PoolDeltaF" title="bfbrain.AL_Metrics.PoolDeltaF"><code class="xref py py-class docutils literal notranslate"><span class="pre">PoolDeltaF</span></code></a> or <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.UnlabelledDeltaF" title="bfbrain.AL_Metrics.UnlabelledDeltaF"><code class="xref py py-class docutils literal notranslate"><span class="pre">UnlabelledDeltaF</span></code></a> metric and stops active learning if the estimated change in the <span class="math notranslate nohighlight">\(F_1\)</span> score computed by those metrics doesn’t achieve a new minimum for a specified number of rounds, given by the attribute <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.DeltaFNotDecreasing.patience" title="bfbrain.AL_Metrics.DeltaFNotDecreasing.patience"><code class="xref py py-attr docutils literal notranslate"><span class="pre">patience</span></code></a>.</p></li>
</ul>
<p>As a simple use case, we can implement the <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.FScoreNotImproving" title="bfbrain.AL_Metrics.FScoreNotImproving"><code class="xref py py-class docutils literal notranslate"><span class="pre">FScoreNotImproving</span></code></a> stopping criterion in our training script by modifying our call to <a class="reference internal" href="bfbrain.html#id7" title="bfbrain.BFB_Learner.BFBLearner.AL_loop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AL_loop</span></code></a> in the following manner:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">bfbrain</span> <span class="kn">import</span> <span class="n">FScoreNotImproving</span>

<span class="n">AL</span><span class="o">.</span><span class="n">AL_loop</span><span class="p">(</span><span class="s1">&#39;saved_AL_stopping&#39;</span><span class="p">,</span> <span class="n">nstop</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">stopping_cond</span> <span class="o">=</span> <span class="n">FScoreNotImproving</span><span class="p">(</span><span class="n">metric_name</span> <span class="o">=</span> <span class="n">AL</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">patience</span> <span class="o">=</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>If we don’t want to wait hours for a full training script to complete again, we can also study a stopping criterion by computing the active learning iteration it <em>would</em> have stopped the active learning
loop of an already-trained <a class="reference internal" href="bfbrain.html#bfbrain.BFB_Learner.BFBLearner" title="bfbrain.BFB_Learner.BFBLearner"><code class="xref py py-class docutils literal notranslate"><span class="pre">BFBLearner</span></code></a> instance, using <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition.find_stopping_index" title="bfbrain.AL_Metrics.StoppingCondition.find_stopping_index"><code class="xref py py-meth docutils literal notranslate"><span class="pre">StoppingCondition.find_stopping_index</span></code></a>. As an example,
we can write</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">bfbrain</span> <span class="kn">import</span> <span class="n">FScoreNotImproving</span>

<span class="n">FScoreNotImproving</span><span class="p">(</span><span class="n">metric_name</span> <span class="o">=</span> <span class="n">AL</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">patience</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">find_stopping_index</span><span class="p">({</span><span class="n">metric</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">metric</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">AL</span><span class="o">.</span><span class="n">metrics</span><span class="p">})</span>
</pre></div>
</div>
<p>to find the index at which this stopping criterion would have stopped AL’s original training. If the above method returns -1, it means that the stopping condition would not have stopped training early.</p>
<p>Finally, custom stopping criteria can be created by writing new classes which inherit from <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition" title="bfbrain.AL_Metrics.StoppingCondition"><code class="xref py py-class docutils literal notranslate"><span class="pre">StoppingCondition</span></code></a> or creating <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition" title="bfbrain.AL_Metrics.StoppingCondition"><code class="xref py py-class docutils literal notranslate"><span class="pre">StoppingCondition</span></code></a>
objects with different arguments in the constructor– in either case, it is highly recommended to inspect <a class="reference internal" href="bfbrain.html#bfbrain.AL_Metrics.StoppingCondition" title="bfbrain.AL_Metrics.StoppingCondition"><code class="xref py py-class docutils literal notranslate"><span class="pre">the</span> <span class="pre">StoppingCondition</span> <span class="pre">documentation</span></code></a> before attempting to do so.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id14" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id5">2</a>,<a role="doc-backlink" href="#id13">3</a>)</span>
<p>Wojcik, G. N., in Preparation [arXiv:2309.XXXXX]</p>
</aside>
<aside class="footnote brackets" id="id15" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p>Gal, Y., &amp; Ghahramani, Z. (2016, June). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning (pp. 1050-1059). PMLR.</p>
</aside>
<aside class="footnote brackets" id="id16" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">3</a><span class="fn-bracket">]</span></span>
<p>Houlsby, N., Huszár, F., Ghahramani, Z., &amp; Lengyel, M. (2011). Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745.</p>
</aside>
<aside class="footnote brackets" id="id17" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id9">2</a>)</span>
<p>Shannon, C. E., (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, vol. 27, no. 3, pp.379-423.</p>
</aside>
<aside class="footnote brackets" id="id18" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id10">2</a>)</span>
<p>Depeweg, S., Hernandez-Lobato, J. M., Doshi-Velez, F., &amp; Udluft, S. (2018, July). Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning. In International Conference on Machine Learning (pp. 1184-1193). PMLR.</p>
</aside>
<aside class="footnote brackets" id="id19" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">6</a><span class="fn-bracket">]</span></span>
<p>Kwon, Y., Won J., Kim B. J., &amp; Paik M. C. (2020). Uncertainty quantification using Bayesian neural networks in classification: Application to biomedical image segmentation. Computational Statistics &amp; Data Analysis, vol. 142, pp. 106816.</p>
</aside>
<aside class="footnote brackets" id="id20" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">7</a><span class="fn-bracket">]</span></span>
<p>Ducoffe, M., &amp; Precioso, F. (2015). Qbdc: query by dropout committee for training deep supervised architecture. arXiv preprint arXiv:1511.06412.</p>
</aside>
</aside>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">bfbrain</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorial.html">Tutorial and User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="data_manager.html">Tutorial Step 1: Oracle and Data Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="classifier.html">Tutorial Step 2: Initializing the Classifier</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tutorial Step 3: Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="analysis.html">Tutorial Step 4: Analysis</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">BFBrain</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="tutorial.html">Tutorial and User Guide</a><ul>
      <li>Previous: <a href="classifier.html" title="previous chapter">Tutorial Step 2: Initializing the Classifier</a></li>
      <li>Next: <a href="analysis.html" title="next chapter">Tutorial Step 4: Analysis</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, George Wojcik.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.2.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/training.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>